---
- name: Provision DO + Install HA K8s with Kubespray + Addons (no DNS automation)
  hosts: localhost
  connection: local
  gather_facts: false

  vars_files:
    - ./group_vars/all.yml

  vars:
    workspace: "{{ playbook_dir }}"
    kubespray_dir: "{{ workspace }}/kubespray"
    inventory_dir: "{{ kubespray_dir }}/inventory/{{ cluster_name }}"
    kubeconfig_path: "{{ lookup('env','HOME') + '/.kube/config' }}"
    pod_network_cidr: "10.233.64.0/18"
    service_cidr: "10.233.0.0/18"
    k8s_version: "v1.29.6"

  pre_tasks:
    - name: Validate DigitalOcean token present
      ansible.builtin.assert:
        that:
          - do_token | length > 0
        fail_msg: "DO_TOKEN environment variable is required."

    - name: Install base system deps
      ansible.builtin.apt:
        name:
          - git
          - python3-pip
          - python3-venv
          - sshpass
          - rsync
          - jq
        update_cache: true
      become: true

    - name: Install Python packages
      ansible.builtin.pip:
        name:
          - "ansible==9.7.0"
          - "kubernetes==29.0.0"
          - "pyyaml"
          - "jinja2"
        extra_args: --user

    - name: Install Ansible collections
      ansible.builtin.command: >
        {{ lookup('env','HOME') + '/.local/bin/ansible-galaxy' }}
        collection install community.digitalocean kubernetes.core community.general
      register: galaxy_out
      changed_when: galaxy_out.rc == 0

  tasks:
    # -------------------- Create droplets (no backrefs) --------------------
    - name: Create control-plane droplets
      community.digitalocean.digital_ocean_droplet:
        state: present
        oauth_token: "{{ do_token }}"
        name: "{{ '%s-cp-%s' | format(basename, item) }}"
        region: "{{ region }}"
        image: "{{ image }}"
        size: "{{ size_cp }}"
        monitoring: true
        ipv6: false
        backups: false
        ssh_keys: ["{{ ssh_key_id }}"]
        tags: ["{{ cluster_name }}", "role:cp"]
      loop: "{{ range(1, (cp_count | int) + 1) | list }}"

    - name: Create worker droplets (if any)
      community.digitalocean.digital_ocean_droplet:
        state: present
        oauth_token: "{{ do_token }}"
        name: "{{ '%s-wk-%s' | format(basename, item) }}"
        region: "{{ region }}"
        image: "{{ image }}"
        size: "{{ size_worker }}"
        monitoring: true
        ipv6: false
        backups: false
        ssh_keys: ["{{ ssh_key_id }}"]
        tags: ["{{ cluster_name }}", "role:worker"]
      loop: "{{ range(1, (worker_count | int) + 1) | list }}"
      when: worker_count | int > 0

    - name: Wait a bit for droplets to boot
      ansible.builtin.pause:
        seconds: "{{ post_create_wait_seconds | int }}"

    # -------------------- Discover droplets (robust shape handling) --------------------
    - name: Gather droplets info (normalize output shape)
      community.digitalocean.digital_ocean_droplet_info:
        oauth_token: "{{ do_token }}"
      register: do_droplets_all

    - name: Normalize droplets list
      ansible.builtin.set_fact:
        all_droplets: >-
          {{ (do_droplets_all.data.droplets
              if (do_droplets_all.data is mapping and ('droplets' in do_droplets_all.data))
              else do_droplets_all.data) | default([], true) }}

    - name: Filter droplets for this cluster by tag
      ansible.builtin.set_fact:
        cluster_droplets: "{{ all_droplets | selectattr('tags','contains', cluster_name) | list }}"

    - name: Extract masters and workers with names/ids/public IPs
      ansible.builtin.set_fact:
        masters: >-
          {{ {'list': cluster_droplets}
             | json_query("list[?contains(tags, 'role:cp')].{name:name, id:id, ip: networks.v4[?type=='public']|[0].ip_address}") }}
        workers: >-
          {{ {'list': cluster_droplets}
             | json_query("list[?contains(tags, 'role:worker')].{name:name, id:id, ip: networks.v4[?type=='public']|[0].ip_address}") }}

    - name: Print server IP addresses
      ansible.builtin.debug:
        msg:
          - "Masters names: {{ masters | map(attribute='name') | list }}"
          - "Masters IPs:   {{ masters | map(attribute='ip') | list }}"
          - "Workers names: {{ workers | map(attribute='name') | list }}"
          - "Workers IPs:   {{ workers | map(attribute='ip') | list }}"

    # -------------------- API Load Balancer (wait with `until` on a single task) --------------------
    - name: Create DO Load Balancer for kube-apiserver
      community.digitalocean.digital_ocean_load_balancer:
        state: present
        oauth_token: "{{ do_token }}"
        name: "{{ api_lb_name }}"
        region: "{{ region }}"
        droplet_ids: "{{ masters | map(attribute='id') | list }}"
        forwarding_rules:
          - entry_protocol: tcp
            entry_port: 6443
            target_protocol: tcp
            target_port: 6443
        health_check:
          protocol: tcp
          port: 6443
          check_interval_seconds: 10
          response_timeout_seconds: 5
          unhealthy_threshold: 3
          healthy_threshold: 5

    - name: Get LB info until API LB has an address
      community.digitalocean.digital_ocean_load_balancer_info:
        oauth_token: "{{ do_token }}"
      register: do_lbs
      until: >
        (
          (
            (do_lbs.data.load_balancers
              if (do_lbs.data is mapping and ('load_balancers' in do_lbs.data))
              else do_lbs.data) | default([], true)
            | selectattr('name','equalto', api_lb_name) | list | length
          ) > 0
        )
        and
        (
          (
            (
              ( (do_lbs.data.load_balancers
                    if (do_lbs.data is mapping and ('load_balancers' in do_lbs.data))
                    else do_lbs.data) | default([], true)
              | selectattr('name','equalto', api_lb_name) | list | first ) | default({})
            ).ip | default('') | length > 0
          )
          or
          (
            (
              ( (do_lbs.data.load_balancers
                    if (do_lbs.data is mapping and ('load_balancers' in do_lbs.data))
                    else do_lbs.data) | default([], true)
              | selectattr('name','equalto', api_lb_name) | list | first ) | default({})
            ).hostname | default('') | length > 0
          )
        )
      retries: 30
      delay: 10
      changed_when: false

    - name: Normalize LB list
      ansible.builtin.set_fact:
        lb_list: >-
          {{ (do_lbs.data.load_balancers
              if (do_lbs.data is mapping and ('load_balancers' in do_lbs.data))
              else do_lbs.data) | default([], true) }}

    - name: Pick API LB record by name
      ansible.builtin.set_fact:
        api_lb_rec: "{{ (lb_list | selectattr('name','equalto', api_lb_name) | list | first) | default({}) }}"

    - name: Set apiserver LB address (IP if present, else hostname)
      ansible.builtin.set_fact:
        apiserver_lb_addr: >-
          {{ (api_lb_rec.ip | default('')) if ((api_lb_rec.ip | default('')) | length > 0)
            else (api_lb_rec.hostname | default('')) }}

    - name: Get LB info until API LB has an address
      community.digitalocean.digital_ocean_load_balancer_info:
        oauth_token: "{{ do_token }}"
      register: do_lbs
      until: >
        (
          (
            (do_lbs.data.load_balancers
              if (do_lbs.data is mapping and ('load_balancers' in do_lbs.data))
              else do_lbs.data) | default([], true)
          | selectattr('name','equalto', api_lb_name) | list | length
          ) > 0
        )
        and
        (
          (
            (
              (do_lbs.data.load_balancers
                if (do_lbs.data is mapping and ('load_balancers' in do_lbs.data))
                else do_lbs.data) | default([], true)
            | selectattr('name','equalto', api_lb_name) | list | first ) | default({})
          ).ip | default('') | length > 0
          or
          (
            (
              (do_lbs.data.load_balancers
                if (do_lbs.data is mapping and ('load_balancers' in do_lbs.data))
                else do_lbs.data) | default([], true)
            | selectattr('name','equalto', api_lb_name) | list | first ) | default({})
          ).hostname | default('') | length > 0
        )
      retries: 30
      delay: 10
      changed_when: false

    # -------------------- Kubespray inventory & config --------------------
    - name: Clone Kubespray
      ansible.builtin.git:
        repo: "{{ kubespray_repo }}"
        dest: "{{ kubespray_dir }}"
        version: "{{ kubespray_branch }}"
        depth: 1

    - name: Create inventory directory
      ansible.builtin.file:
        path: "{{ inventory_dir }}"
        state: directory
        mode: "0755"

    - name: Ensure group_vars directory exists
      ansible.builtin.file:
        path: "{{ inventory_dir }}/group_vars"
        state: directory
        mode: "0755"

    - name: Ensure group_vars/k8s_cluster directory exists
      ansible.builtin.file:
        path: "{{ inventory_dir }}/group_vars/k8s_cluster"
        state: directory
        mode: "0755"

    - name: Write hosts.yaml
      ansible.builtin.copy:
        dest: "{{ inventory_dir }}/hosts.yaml"
        mode: "0644"
        content: |
          all:
            hosts:
          {% for m in masters %}
              {{ m.name }}:
                ansible_host: {{ m.ip }}
                ip: {{ m.ip }}
                access_ip: {{ m.ip }}
                ansible_user: root
          {% endfor %}
          {% for w in workers %}
              {{ w.name }}:
                ansible_host: {{ w.ip }}
                ip: {{ w.ip }}
                access_ip: {{ w.ip }}
                ansible_user: root
          {% endfor %}
            children:
              kube_control_plane:
                hosts:
          {% for m in masters %}
                  {{ m.name }}: {}
          {% endfor %}
              kube_node:
                hosts:
          {% for m in masters %}
                  {{ m.name }}: {}
          {% endfor %}
          {% for w in workers %}
                  {{ w.name }}: {}
          {% endfor %}
              etcd:
                hosts:
          {% for m in masters %}
                  {{ m.name }}: {}
          {% endfor %}
              k8s_cluster:
                children:
                  kube_control_plane: {}
                  kube_node: {}
              calico_rr: {}

    - name: Configure k8s-cluster.yml (DigitalOcean external CCM + LB address)
      ansible.builtin.copy:
        dest: "{{ inventory_dir }}/group_vars/k8s_cluster/k8s-cluster.yml"
        mode: "0644"
        content: |
          kube_version: "{{ k8s_version }}"
          kube_network_plugin: calico
          kube_pods_subnet: "{{ pod_network_cidr }}"
          kube_service_addresses: "{{ service_cidr }}"
          cloud_provider: external
          external_cloud_provider: digitalocean
          kubelet_loadbalancer_mode: "ipvs"
          loadbalancer_apiserver:
            address: "{{ apiserver_lb_addr }}"
            port: 6443

    - name: Enable dashboard & helm
      ansible.builtin.copy:
        dest: "{{ inventory_dir }}/group_vars/k8s_cluster/addons.yml"
        mode: "0644"
        content: |
          dashboard_enabled: true
          helm_enabled: true

    - name: Disable SSH host key checking locally
      ansible.builtin.lineinfile:
        path: "{{ lookup('env','HOME') + '/.ssh/config' }}"
        line: "Host *\n    StrictHostKeyChecking no\n    UserKnownHostsFile=/dev/null"
        create: true
        mode: "0600"

    - name: Run Kubespray
      ansible.builtin.command: >
        {{ lookup('env','HOME') + '/.local/bin/ansible-playbook' }}
        -i {{ inventory_dir }}/hosts.yaml
        -b -u root cluster.yml
      args:
        chdir: "{{ kubespray_dir }}"

    # --- kubeconfig on the runner (localhost) ---
    - name: Ensure ~/.kube exists on localhost
      ansible.builtin.file:
        path: "{{ kubeconfig_path | dirname }}"
        state: directory
        mode: "0700"

    - name: Fetch kubeconfig from first master
      ansible.builtin.command: >
        scp -o StrictHostKeyChecking=no
        root@{{ (masters | first).ip }}:/etc/kubernetes/admin.conf
        {{ kubeconfig_path }}
      changed_when: true

    - name: Rewrite kubeconfig server to the DO LB address
      ansible.builtin.replace:
        path: "{{ kubeconfig_path }}"
        regexp: '^(\s*server:\s*)https://.*:6443$'
        replace: '\1https://{{ apiserver_lb_addr }}:6443'

    - name: Install kubectl client on localhost (match cluster version)
      become: true
      ansible.builtin.get_url:
        url: "https://dl.k8s.io/release/{{ k8s_version }}/bin/linux/amd64/kubectl"
        dest: /usr/local/bin/kubectl
        mode: "0755"

    - name: Sanity check kubectl can reach the cluster
      ansible.builtin.command: >
        /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }} get nodes -o wide
      register: kubectl_nodes
      changed_when: false
      retries: 12
      delay: 10
      until: kubectl_nodes.rc == 0

    - name: Show cluster nodes (for visibility)
      ansible.builtin.debug:
        var: kubectl_nodes.stdout_lines

    - name: Ensure kube dir perms
      ansible.builtin.file:
        path: "{{ kubeconfig_path | dirname }}"
        state: directory
        mode: "0700"

    # -------------------- Addons --------------------
    - name: Download ingress-nginx manifest (cloud provider)
      ansible.builtin.get_url:
        url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
        dest: ./ingress-nginx.yaml

    - name: Apply ingress-nginx
      kubernetes.core.k8s:
        kubeconfig: "{{ kubeconfig_path }}"
        state: present
        src: ./ingress-nginx.yaml

    # --- cert-manager (pinned manifest for k8s 1.28) ---
    - name: Set cert-manager version compatible with {{ k8s_version }}
      ansible.builtin.set_fact:
        cert_manager_version: "v1.13.3"

    - name: Download cert-manager manifest {{ cert_manager_version }}
      ansible.builtin.get_url:
        url: "https://github.com/cert-manager/cert-manager/releases/download/{{ cert_manager_version }}/cert-manager.yaml"
        dest: "/tmp/cert-manager-{{ cert_manager_version }}.yaml"
        mode: "0644"

    - name: Create cert-manager namespace (idempotent)
      ansible.builtin.command: >
        /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
        create namespace cert-manager
      register: cm_ns
      failed_when: cm_ns.rc not in [0,1]
      changed_when: cm_ns.rc == 0

    - name: Apply cert-manager {{ cert_manager_version }}
      ansible.builtin.command: >
        /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
        apply -f /tmp/cert-manager-{{ cert_manager_version }}.yaml
      register: cm_apply
      changed_when: "'created' in cm_apply.stdout or 'configured' in cm_apply.stdout"

    # --- Wait for cert-manager CRDs to register (unchanged) ---
    - name: Wait for cert-manager CRDs to register
      ansible.builtin.command: >
        /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
        get crd certificates.cert-manager.io
      register: cm_crd_check
      changed_when: false
      retries: 40
      delay: 5
      until: cm_crd_check.rc == 0

    # --- Robust rollout with auto-remediation + diagnostics ---
    - name: Rollout cert-manager with remediation and diagnostics
      block:
        - name: Wait for cert-manager pods to appear
          ansible.builtin.command: >
            /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
            -n cert-manager get pods
          register: cm_pods_initial
          changed_when: false
          retries: 20
          delay: 6
          until: cm_pods_initial.rc == 0

        - name: Try rollout of all cert-manager deployments
          ansible.builtin.command: >
            /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
            -n cert-manager rollout status deploy/cert-manager
            --timeout=240s
          register: cm_ctrl_rollout
          changed_when: false
          failed_when: false

        - name: Record rollout failed?
          ansible.builtin.set_fact:
            cm_rollout_failed: "{{ cm_ctrl_rollout.rc != 0 }}"

        - name: Collect pod JSON (for analysis)
          ansible.builtin.command: >
            /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
            -n cert-manager get pods -o json
          register: cm_pods_json
          changed_when: false

        - name: Detect ImagePullBackOff on controller
          ansible.builtin.set_fact:
            cm_pull_issue: "{{ 'ImagePullBackOff' in cm_pods_json.stdout }}"

        - name: If ImagePullBackOff, retarget images to ghcr.io and retry rollout
          when: cm_rollout_failed and cm_pull_issue
          block:
            - name: Patch controller image to ghcr.io
              ansible.builtin.command: >
                /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
                -n cert-manager set image deploy/cert-manager
                cert-manager-controller=ghcr.io/cert-manager/cert-manager-controller:{{ cert_manager_version }}
              changed_when: true

            - name: Patch cainjector image to ghcr.io
              ansible.builtin.command: >
                /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
                -n cert-manager set image deploy/cert-manager-cainjector
                cert-manager-cainjector=ghcr.io/cert-manager/cert-manager-cainjector:{{ cert_manager_version }}
              changed_when: true

            - name: Patch webhook image to ghcr.io
              ansible.builtin.command: >
                /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
                -n cert-manager set image deploy/cert-manager-webhook
                cert-manager-webhook=ghcr.io/cert-manager/cert-manager-webhook:{{ cert_manager_version }}
              changed_when: true

            - name: Wait for image repull
              ansible.builtin.pause:
                seconds: 10

            - name: Retry rollout after image patch
              ansible.builtin.command: >
                /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
                -n cert-manager rollout status deploy/cert-manager --timeout=300s
              register: cm_ctrl_rollout_after_patch
              changed_when: false
              retries: 3
              delay: 15
              until: cm_ctrl_rollout_after_patch.rc == 0
              failed_when: false

        - name: If rollout still failed after remediation, try one more time with extended timeout
          when: cm_rollout_failed and (cm_ctrl_rollout_after_patch is not defined or cm_ctrl_rollout_after_patch.rc != 0)
          block:
            - name: Final rollout attempt with extended timeout
              ansible.builtin.command: >
                /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
                -n cert-manager rollout status deploy/cert-manager --timeout=360s
              register: cm_ctrl_final
              changed_when: false
              retries: 2
              delay: 20
              until: cm_ctrl_final.rc == 0
              failed_when: false

        - name: Ensure cainjector rollout (post controller OK)
          ansible.builtin.command: >
            /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
            -n cert-manager rollout status deploy/cert-manager-cainjector --timeout=240s
          register: cm_cainj_rollout
          changed_when: false
          retries: 5
          delay: 10
          until: cm_cainj_rollout.rc == 0
          failed_when: false

        - name: Ensure webhook rollout (post controller OK)
          ansible.builtin.command: >
            /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
            -n cert-manager rollout status deploy/cert-manager-webhook --timeout=240s
          register: cm_webhook_rollout
          changed_when: false
          retries: 5
          delay: 10
          until: cm_webhook_rollout.rc == 0
          failed_when: false

        - name: Check if all deployments succeeded
          ansible.builtin.set_fact:
            all_deployments_ok: "{{ (cm_ctrl_rollout.rc == 0 or (cm_ctrl_rollout_after_patch is defined and cm_ctrl_rollout_after_patch.rc == 0) or (cm_ctrl_final is defined and cm_ctrl_final.rc == 0)) and cm_cainj_rollout.rc == 0 and cm_webhook_rollout.rc == 0 }}"

        - name: Final check - at least verify pods are running
          when: not all_deployments_ok | bool
          block:
            - name: Wait extra time for pods to stabilize
              ansible.builtin.pause:
                seconds: 30

            - name: Check running cert-manager pods
              ansible.builtin.command: >
                /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
                -n cert-manager get pods --field-selector=status.phase=Running -o json
              register: running_pods_check
              changed_when: false

            - name: Count running pods
              ansible.builtin.set_fact:
                running_pods_count: "{{ (running_pods_check.stdout | from_json)['items'] | length }}"

            - name: Override success if we have at least 3 running pods
              ansible.builtin.set_fact:
                all_deployments_ok: "{{ running_pods_count | int >= 3 }}"

      rescue:
        - name: Show cert-manager pods
          ansible.builtin.command: >
            /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
            -n cert-manager get pods -o wide
          register: cm_pods
          changed_when: false
          failed_when: false

        - name: Describe cert-manager deployment
          ansible.builtin.command: >
            /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
            -n cert-manager describe deploy/cert-manager
          register: cm_deploy_desc
          changed_when: false
          failed_when: false

        - name: Tail controller logs
          ansible.builtin.command: >
            /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
            -n cert-manager logs deploy/cert-manager --tail=200
          register: cm_logs
          changed_when: false
          failed_when: false

        - name: Show recent namespace events
          ansible.builtin.command: >
            /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
            -n cert-manager get events --sort-by=.lastTimestamp | tail -n 80
          register: cm_events
          changed_when: false
          failed_when: false

        - name: Check if any pods are actually running despite rollout failure
          ansible.builtin.command: >
            /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
            -n cert-manager get pods --field-selector=status.phase=Running -o json
          register: rescue_running_pods
          changed_when: false
          failed_when: false

        - name: Count running pods in rescue
          ansible.builtin.set_fact:
            rescue_running_count: "{{ (rescue_running_pods.stdout | from_json)['items'] | length }}"
          when: rescue_running_pods.rc == 0

        - name: Diagnostics (cert-manager)
          ansible.builtin.debug:
            msg:
              - "========================================"
              - "cert-manager rollout encountered issues"
              - "========================================"
              - "Running pods found: {{ rescue_running_count | default(0) }}"
              - ""
              - "pods:"
              - "{{ cm_pods.stdout_lines | default([]) }}"
              - ""
              - "deploy describe:"
              - "{{ cm_deploy_desc.stdout_lines | default([]) }}"
              - ""
              - "logs (controller):"
              - "{{ cm_logs.stdout_lines | default([]) }}"
              - ""
              - "recent events:"
              - "{{ cm_events.stdout_lines | default([]) }}"

        - name: Fail cert-manager rollout only if no pods are running
          ansible.builtin.fail:
            msg: "cert-manager failed to roll out and no pods are running. See diagnostics above."
          when: (rescue_running_count | default(0) | int) < 3

    # --- Diagnostics if rollout still flaky ---
    - name: Show cert-manager pods (debug)
      ansible.builtin.command: >
        /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
        -n cert-manager get pods -o wide
      register: cm_pods
      changed_when: false
      failed_when: false

    - name: Show cert-manager events (debug)
      ansible.builtin.command: >
        /usr/local/bin/kubectl --kubeconfig {{ kubeconfig_path }}
        -n cert-manager get events --sort-by=.lastTimestamp | tail -n 50
      register: cm_events
      changed_when: false
      failed_when: false

    - name: Debug output
      ansible.builtin.debug:
        msg:
          - "cert-manager pods:"
          - "{{ cm_pods.stdout_lines | default([]) }}"
          - "recent events:"
          - "{{ cm_events.stdout_lines | default([]) }}"
      when: cm_webhook_rollout is failed

    - name: Create ClusterIssuer (Let's Encrypt)
      kubernetes.core.k8s:
        kubeconfig: "{{ kubeconfig_path }}"
        state: present
        definition:
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: letsencrypt-production
          spec:
            acme:
              email: "{{ email_acme }}"
              server: https://acme-v02.api.letsencrypt.org/directory
              privateKeySecretRef:
                name: le-account-key
              solvers:
                - http01:
                    ingress:
                      class: nginx

    - name: Download ArgoCD install manifest
      ansible.builtin.get_url:
        url: https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
        dest: ./argocd.yaml

    - name: Apply ArgoCD
      kubernetes.core.k8s:
        kubeconfig: "{{ kubeconfig_path }}"
        state: present
        src: ./argocd.yaml

    - name: Expose ArgoCD with Ingress + TLS
      kubernetes.core.k8s:
        kubeconfig: "{{ kubeconfig_path }}"
        state: present
        definition:
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: argocd-server
            namespace: argocd
            annotations:
              kubernetes.io/ingress.class: nginx
              cert-manager.io/cluster-issuer: letsencrypt-production
              nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
              nginx.ingress.kubernetes.io/ssl-passthrough: "false"
          spec:
            tls:
              - hosts: ["{{ argocd_domain }}"]
                secretName: argocd-tls
            rules:
              - host: "{{ argocd_domain }}"
                http:
                  paths:
                    - path: /
                      pathType: Prefix
                      backend:
                        service:
                          name: argocd-server
                          port:
                            number: 443

    - name: Download Kubernetes Dashboard manifest
      ansible.builtin.get_url:
        url: https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
        dest: ./k8s-dashboard.yaml

    - name: Apply Kubernetes Dashboard
      kubernetes.core.k8s:
        kubeconfig: "{{ kubeconfig_path }}"
        state: present
        src: ./k8s-dashboard.yaml

    - name: Create admin SA & RBAC for Dashboard
      kubernetes.core.k8s:
        kubeconfig: "{{ kubeconfig_path }}"
        state: present
        definition:
          - apiVersion: v1
            kind: ServiceAccount
            metadata:
              name: admin-user
              namespace: kubernetes-dashboard
          - apiVersion: rbac.authorization.k8s.io/v1
            kind: ClusterRoleBinding
            metadata:
              name: admin-user
            subjects:
              - kind: ServiceAccount
                name: admin-user
                namespace: kubernetes-dashboard
            roleRef:
              kind: ClusterRole
              name: cluster-admin
              apiGroup: rbac.authorization.k8s.io

    - name: Expose Dashboard with Ingress + TLS
      kubernetes.core.k8s:
        kubeconfig: "{{ kubeconfig_path }}"
        state: present
        definition:
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: kubernetes-dashboard
            namespace: kubernetes-dashboard
            annotations:
              kubernetes.io/ingress.class: nginx
              cert-manager.io/cluster-issuer: letsencrypt-production
              nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
              nginx.ingress.kubernetes.io/ssl-redirect: "true"
          spec:
            tls:
              - hosts: ["{{ dashboard_domain }}"]
                secretName: dashboard-tls
            rules:
              - host: "{{ dashboard_domain }}"
                http:
                  paths:
                    - path: /
                      pathType: Prefix
                      backend:
                        service:
                          name: kubernetes-dashboard
                          port:
                            number: 443

    # -------------------- Wait for Ingress LB & Prompt for DNS --------------------
    - name: Wait for ingress-nginx external IP/hostname to be assigned
      ansible.builtin.command: kubectl -n ingress-nginx get svc ingress-nginx-controller -o json
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: svcjson
      changed_when: false
      retries: 60
      delay: 10
      until: >
        (svcjson.stdout | from_json).status.loadBalancer.ingress is defined and
        ((svcjson.stdout | from_json).status.loadBalancer.ingress | length) > 0 and
        (((svcjson.stdout | from_json).status.loadBalancer.ingress[0].ip | default('')) != '' or
         ((svcjson.stdout | from_json).status.loadBalancer.ingress[0].hostname | default('')) != '')

    - name: Extract ingress external address
      ansible.builtin.set_fact:
        ingress_ip: "{{ (svcjson.stdout | from_json).status.loadBalancer.ingress[0].ip | default('') }}"
        ingress_hostname: "{{ (svcjson.stdout | from_json).status.loadBalancer.ingress[0].hostname | default('') }}"
        ingress_addr: "{{ (ingress_ip | length > 0) | ternary(ingress_ip, ingress_hostname) }}"

    - name: Show DNS instructions
      ansible.builtin.debug:
        msg:
          - "Ingress external address: {{ ingress_addr }}"
          - "Please create A/CNAME records pointing to this address:"
          - "  - {{ argocd_domain }} -> {{ ingress_addr }}"
          - "  - {{ dashboard_domain }} -> {{ ingress_addr }}"

    - name: Confirm DNS added or wait
      ansible.builtin.pause:
        prompt: "Add the DNS records now, then press Enter to continue (or it will wait {{ dns_wait_seconds }}s if dns_wait_mode: seconds)."
        seconds: "{{ 0 if dns_wait_mode == 'prompt' else dns_wait_seconds | int }}"

    # -------------------- Outputs --------------------
    - name: Read ArgoCD initial admin password
      ansible.builtin.command: kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath='{.data.password}'
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: argopass_b64
      changed_when: false

    - name: Print access info
      ansible.builtin.debug:
        msg:
          - "API LB (6443): {{ apiserver_lb_addr }}"
          - "Ingress: {{ ingress_addr }}"
          - "ArgoCD: https://{{ argocd_domain }}  (admin / {{ (argopass_b64.stdout | b64decode) if argopass_b64.stdout else 'N/A' }})"
          - "Dashboard: https://{{ dashboard_domain }} (token printed below)"

    - name: Print Dashboard token
      ansible.builtin.command: kubectl -n kubernetes-dashboard create token admin-user
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: dash_token
      changed_when: false

    - name: Dashboard token (copy this for login)
      ansible.builtin.debug:
        msg: "{{ dash_token.stdout }}"
